{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81193f21-f4a2-462e-a98c-ff85b0464209",
   "metadata": {},
   "source": [
    "# ndvi_smol\n",
    "\n",
    "### Here we gather Sentinel-2 data to calculate plot ndvi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f59cb-a5b0-468a-841b-aba8a4ddcc4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # For Sentinel-2\n",
    "# # !pip install sentinelsat\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install sentinelsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58760e9-71ac-4d88-a952-905efeb58b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "# os.chdir('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "from scipy.stats import zscore\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d523c4a-8a54-4b63-b158-4a5828db4b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_indices(image):\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('ndvi')\n",
    "    evi = image.expression(\n",
    "        '2.5 * ((NIR - RED) / (NIR + 6*RED - 7.5*BLUE + 1))',\n",
    "        {'NIR': image.select('B8'), 'RED': image.select('B4'), 'BLUE': image.select('B2')}\n",
    "    ).rename('evi')\n",
    "    ndwi = image.normalizedDifference(['B8', 'B11']).rename('ndwi')\n",
    "    savi = image.expression(\n",
    "        '((NIR - RED) / (NIR + RED + 0.5)) * 1.5',\n",
    "        {'NIR': image.select('B8'), 'RED': image.select('B4')}\n",
    "    ).rename('savi')\n",
    "    rendvi = image.normalizedDifference(['B8', 'B5']).rename('rendvi')\n",
    "    mcari2 = image.expression(\n",
    "        '((NIR - RE) - 0.2*(NIR - RED)) * (NIR / RE)',\n",
    "        {'NIR': image.select('B8'), 'RED': image.select('B4'), 'RE': image.select('B5')}\n",
    "    ).rename('mcari2')\n",
    "    \n",
    "    return image.addBands([ndvi, evi, ndwi, savi, rendvi, mcari2])\n",
    "\n",
    "\n",
    "\n",
    "def mask_clouds(img, cloud_prob_threshold=20):\n",
    "    '''Mask clouds using s2cloudless probability band.'''\n",
    "    cloud_prob = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY') \\\n",
    "        .filter(ee.Filter.eq('system:index', img.get('system:index'))).first()\n",
    "    clouds = cloud_prob.gt(cloud_prob_threshold)\n",
    "    mask = clouds.Not()\n",
    "    return img.updateMask(mask)\n",
    "\n",
    "\n",
    "def reduce_region(img, geom, plot_id):\n",
    "    # all EE operations\n",
    "    stats = img.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=geom,\n",
    "        scale=10,\n",
    "        bestEffort=True,\n",
    "        maxPixels=1e13\n",
    "    )\n",
    "    props = {k: stats.get(k) for k in [\n",
    "        'ndvi',#'evi','ndwi','savi','rendvi','mcari2'\n",
    "    ]}\n",
    "    props['date'] = img.date().format('YYYY-MM-dd')\n",
    "    props['plot_id'] = plot_id\n",
    "    return ee.Feature(None, props)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1873afa-bff4-421e-a15d-f4692349815a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Here we initialize our access to the satellite data download. \n",
    "## Requires authentication via browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee2f25-15c6-4bb9-b659-a1b7b8780573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# load the vineyard polygon (GeoJSON)\n",
    "vineyard = pd.read_pickle('../data/polygons/RegressionRidge_smol_smol.pkl')\n",
    "\n",
    "\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "# your existing GeoDataFrame\n",
    "gdf = vineyard.copy()\n",
    "\n",
    "rows = []\n",
    "for idx, row in gdf.iterrows():\n",
    "    geom = row.geometry\n",
    "    parent = row['plot_id']\n",
    "    \n",
    "    if isinstance(geom, Polygon):\n",
    "        rows.append({\"geometry\": geom, \"parent_plot\": parent})\n",
    "    elif isinstance(geom, MultiPolygon):\n",
    "        for poly in geom.geoms:\n",
    "            rows.append({\"geometry\": poly, \"parent_plot\": parent})\n",
    "\n",
    "# create new flattened GeoDataFrame\n",
    "vineyard_flat = gpd.GeoDataFrame(rows, crs=gdf.crs)\n",
    "\n",
    "\n",
    "\n",
    "geoms = [ee.Geometry.Polygon(list(p.exterior.coords)) for p in vineyard_flat.geometry]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204b652-b5e8-4ba9-b8b1-fb29473f29af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37779e75-ebaf-49dd-a2a2-f6c72f2d8b87",
   "metadata": {},
   "source": [
    "### The below code will loop over a given set of years starting in 2016 (earliest available data), search for the polygon coordinates and compute ndvi for our plots. Finally, the data is serialized as a pandas pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c434b89-275e-40d1-8dd5-91955d1b55ce",
   "metadata": {},
   "source": [
    "Set dates to search over. Make sure there is a 'plots' folder for the plot ndvi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d96b1-2a31-4dfd-9c10-2af9a5f7ddc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(2016, 2025)]\n",
    "months = ['01', '12']\n",
    "days = ['01', '31']\n",
    "start_dates = [f'{year}-{months[0]}-{days[0]}' for year in years]\n",
    "end_dates   = [f'{year}-{months[1]}-{days[1]}' for year in years]\n",
    "\n",
    "os.makedirs('data/ndvi/plots', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57ce8d-543e-449d-8423-0042534f9d19",
   "metadata": {},
   "source": [
    "Loop over years and search database for images overlapping our polygons. Then download the and st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb00070-1dff-459a-b667-affbaf545dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Flatten MultiPolygons if necessary ---\n",
    "def shapely_to_ee_feature(poly, plot_id):\n",
    "    \"\"\"Convert a Polygon or MultiPolygon to EE Feature with plot_id property.\"\"\"\n",
    "    if poly.geom_type == 'Polygon':\n",
    "        return ee.Feature(ee.Geometry.Polygon(list(poly.exterior.coords)), {'plot_id': plot_id})\n",
    "    elif poly.geom_type == 'MultiPolygon':\n",
    "        features = []\n",
    "        for p in poly.geoms:\n",
    "            features.append(ee.Feature(ee.Geometry.Polygon(list(p.exterior.coords)), {'plot_id': plot_id}))\n",
    "        return ee.FeatureCollection(features)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported geometry type: {poly.geom_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22760e7c-04a7-48ae-9f7e-0ed9864ad2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff69d25-95ff-4250-8319-142d0ac95ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flatten GeoDataFrame into FeatureCollection\n",
    "ee_features = []\n",
    "for idx, row in tqdm(vineyard_flat.iterrows()):\n",
    "    poly = row.geometry\n",
    "    parent = row.get('parent_plot', f'plot_{idx}')\n",
    "    feat = shapely_to_ee_feature(poly, parent)\n",
    "    \n",
    "    if isinstance(feat, ee.FeatureCollection):\n",
    "        ee_features.extend(feat.toList(feat.size()).getInfo())\n",
    "    else:\n",
    "        ee_features.append(feat.getInfo())\n",
    "        \n",
    "# hex_fc['geometry'] = hex_fc.geometry.buffer(5)  # buffer by 5 meters\n",
    "\n",
    "hex_fc = ee.FeatureCollection(ee_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9634df-f4e5-4a50-b4f4-0a545ec599d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hex_fc_ee = hex_fc.map(lambda f: f.setGeometry(f.geometry().transform('EPSG:4326')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce751d3-9b30-4454-92fc-bfbaee6d95ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for start_date, end_date in zip(start_dates, end_dates):\n",
    "    break\n",
    "    '''\n",
    "    Undo the break statement to download the data\n",
    "    '''\n",
    "    \n",
    "    # fname = f'../data/ndvi/plots/ndvi_{start_date}_to_{end_date}.pkl'\n",
    "    # if os.path.isfile(fname):\n",
    "    #     continue\n",
    "\n",
    "    # Load Sentinel-2 collection and add indices\n",
    "    collection = (\n",
    "        ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "        .filterBounds(hex_fc)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
    "        .select(['B2', 'B4', 'B5', 'B8', 'B11'])\n",
    "        .map(add_indices)\n",
    "    )\n",
    "\n",
    "    # Server-side extraction: map reduce_region over each hex for each image\n",
    "    def extract_features(img):\n",
    "        return hex_fc.map(\n",
    "            lambda f: reduce_region(\n",
    "                img, \n",
    "                f.geometry(), \n",
    "                f.get('plot_id')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Flatten all features across all images\n",
    "    fc = collection.map(extract_features).flatten()\n",
    "\n",
    "    # Export to Google Drive as CSV to avoid memory issues\n",
    "    task = ee.batch.Export.table.toDrive(\n",
    "        collection=fc,\n",
    "        description=f'ndvi_{start_date}_to_{end_date}',\n",
    "        folder=f'ee_ndvi_exports_{start_date}_to_{end_date}',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    task.start()\n",
    "    print(f\"Export started for {start_date} to {end_date}. Check Google Drive folder 'ee_ndvi_exports'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27c3af-3a8e-45cf-af5c-40c0d1bd042a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "code to check on earth engine task status\n",
    "'''\n",
    "# import ee\n",
    "# ee.Initialize()\n",
    "\n",
    "# tasks = ee.batch.Task.list()\n",
    "\n",
    "# for t in tasks:\n",
    "#     print(f\"ID: {t.id}\")\n",
    "#     print(f\"Type: {t.task_type}\")\n",
    "#     print(f\"Description: {t.config.get('description')}\")\n",
    "#     print(f\"State: {t.state}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be957d-f8d0-4f49-b2ae-136c929ded5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi_files = [\n",
    "    'ndvi_2016-01-01_to_2016-12-31.csv',\n",
    "    'ndvi_2017-01-01_to_2017-12-31.csv',\n",
    "    'ndvi_2018-01-01_to_2018-12-31.csv',\n",
    "    'ndvi_2019-01-01_to_2019-12-31.csv',\n",
    "    'ndvi_2020-01-01_to_2020-12-31.csv',\n",
    "    'ndvi_2021-01-01_to_2021-12-31.csv',\n",
    "    'ndvi_2022-01-01_to_2022-12-31.csv',\n",
    "    'ndvi_2023-01-01_to_2023-12-31.csv',\n",
    "    'ndvi_2024-01-01_to_2024-12-31.csv',\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4bdab-2911-437f-8405-91812f645092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dfs = []\n",
    "data_path = '../data/ndvi/plots/'\n",
    "\n",
    "for file_path in tqdm(ndvi_files):\n",
    "\n",
    "    ndvi_path = os.path.join(data_path, file_path)\n",
    "    df = pd.read_csv(ndvi_path)\n",
    "    df = df[['date','ndvi', 'plot_id']]\n",
    "    \n",
    "    dfs.append(df)\n",
    "    \n",
    "df = pd.concat(dfs, axis = 0)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6592b-49ed-4868-a2d9-13a293491544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0f5dc-1153-4de6-8e89-e79295ea1d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def hampel_filter_series(s, window=7, n_sigmas=3):\n",
    "    \"\"\"\n",
    "    Hampel filter for 1D pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "    - s: pd.Series, time-ordered\n",
    "    - window: half-window size (total window = 2*window+1)\n",
    "    - n_sigmas: threshold multiplier\n",
    "    \n",
    "    Returns:\n",
    "    - pd.Series with outliers replaced by NaN\n",
    "    \"\"\"\n",
    "    k = 1.4826  # scale factor for MAD\n",
    "    s_clean = s.copy()\n",
    "\n",
    "    # rolling median and MAD\n",
    "    rolling_med = s.rolling(window=2*window+1, center=True, min_periods=1).median()\n",
    "    rolling_mad = s.rolling(window=2*window+1, center=True, min_periods=1).apply(\n",
    "        lambda x: np.median(np.abs(x - np.median(x))), raw=True\n",
    "    )\n",
    "\n",
    "    # avoid zero MAD\n",
    "    rolling_mad[rolling_mad == 0] = 1e-6\n",
    "\n",
    "    # compute deviations\n",
    "    diff = np.abs(s - rolling_med)\n",
    "    threshold = n_sigmas * k * rolling_mad\n",
    "\n",
    "    # replace outliers with NaN\n",
    "    s_clean[diff > threshold] = np.nan\n",
    "    return s_clean\n",
    "\n",
    "# list of columns to filter\n",
    "cols_to_filter = [\n",
    "    'ndvi', #'evi', 'ndwi', 'savi', 'rendvi', 'mcari2'\n",
    "]\n",
    "\n",
    "for col in tqdm(cols_to_filter):\n",
    "    # apply Hampel filter group-wise\n",
    "    df[col] = df.groupby('plot_id')[col].transform(\n",
    "        lambda s: hampel_filter_series(s, window=7, n_sigmas=2)\n",
    "    )\n",
    "    \n",
    "    # smooth filtered series\n",
    "    df[f'{col}_smooth'] = df.groupby('plot_id')[col].transform(\n",
    "        lambda s: s.rolling(window=7, center=True, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "print(\"Hampel filter and smoothing done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72d013-a974-4dd1-b164-3f77a82c0b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ecc52-7480-49fa-a0ec-c5ade8230d5f",
   "metadata": {},
   "source": [
    "### Let's do a quick stats check here. Figure out how confident we are in these smoothed curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42192e59-bb69-4f47-9152-dfd94529dd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for col in ['ndvi', 'evi', 'ndwi', 'savi', 'rendvi', 'mcari2']:\n",
    "#     df[col + '_residual'] = df[col] - df[col + '_smooth']\n",
    "#     df[col + '_residual_pct'] = (df[col] - df[col + '_smooth']) / df[col + '_smooth'] * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52afc3-a76f-46c0-948c-0c51f1b683b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602ec78-560b-4bd8-9c85-8317963beacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Identify your index columns\n",
    "id_cols = [\"plot_id\", \"date\"]\n",
    "\n",
    "# Raw indices\n",
    "raw_cols = [\n",
    "    \"ndvi\",#\"evi\",\"ndwi\",\"savi\",\"rendvi\",\"mcari2\"\n",
    "]\n",
    "\n",
    "# Smooth versions\n",
    "smooth_cols = ['ndvi_smooth']#[c+\"_smooth\" for c in raw_cols]\n",
    "\n",
    "# Step 1: Fill raw with smoothed, then median per plot, then global median\n",
    "for raw, smooth in zip(raw_cols, smooth_cols):\n",
    "    \n",
    "    df[raw] = df[raw].fillna(df[smooth])  # use smoothed\n",
    "    df[raw] = df.groupby(\"plot_id\")[raw].transform(lambda x: x.fillna(x.median()))\n",
    "    df[raw] = df[raw].fillna(df[raw].median())\n",
    "\n",
    "# Step 2: Fill smooth (only a few NaNs)\n",
    "for col in smooth_cols:\n",
    "    df[col] = df.groupby(\"plot_id\")[col].transform(lambda x: x.fillna(x.median()))\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Step 3: Recompute residuals\n",
    "for raw, smooth in zip(raw_cols, smooth_cols):\n",
    "    resid = raw + \"_residual\"\n",
    "    resid_pct = raw + \"_residual_pct\"\n",
    "    df[resid] = df[raw] - df[smooth]\n",
    "    # To avoid divide-by-zero, use np.where\n",
    "    df[resid_pct] = np.where(df[smooth] != 0, df[resid] / df[smooth], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e8f31-229e-46c7-b9b9-303fadb6e280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89d606-b3a9-421a-ad0b-e5a21eba11d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_id = 23    # example plot\n",
    "year = 2024    # example year\n",
    "\n",
    "df_plot = df[(df['plot_id']==plot_id) & (df['date'].dt.year==year)].sort_values('date')\n",
    "\n",
    "indices = [\n",
    "    'ndvi',#'evi','ndwi','savi','rendvi','mcari2'\n",
    "]\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "for idx in indices:\n",
    "    plt.plot(df_plot['date'], df_plot[idx], 'o-', alpha=0.5, label=f'{idx} raw')\n",
    "    sm = idx + '_smooth'\n",
    "    if sm in df_plot.columns:\n",
    "        plt.plot(df_plot['date'], df_plot[sm], '-', lw=2, label=f'{idx} smooth')\n",
    "    plt.title(f'Plot {plot_id} - {year} index time series')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Index value')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099eb9fa-4c9e-479c-b2b9-6e45ab7b5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_features = [\n",
    "    'ndvi_smooth',\n",
    "                # 'evi_smooth', 'ndwi_smooth', 'savi_smooth', 'rendvi_smooth', 'mcari2_smooth'\n",
    "]\n",
    "\n",
    "df['month'] = df['date'].dt.month\n",
    "df['week'] = df['date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7541dc-b109-44d3-a59d-abddd02e205c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b6795-a1ad-4ee7-93d8-16d51b7ce898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_pickle('ndvi_df_smoothed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03b66f-08d9-422e-a98a-331357e5c1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('ndvi_df_smoothed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcae87-be49-44c9-bbc3-ce687412e8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_orig = df.copy()\n",
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a4490-65a0-43ab-954d-6bffdd8bf13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf969e-93fb-4344-958f-22045d137a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[\n",
    "    (df['week'] > 27) &\n",
    "    (df['week'] < 46)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ec34d-0df5-4a68-b944-bec529759196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stretch_to_daily_per_year(g):\n",
    "    # g is all data for one plot_id\n",
    "    daily_parts = []\n",
    "    for year, g_year in (g.groupby(g['date'].dt.year)):\n",
    "        g_year = g_year.copy()\n",
    "        numeric_cols = g_year.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if 'date' in numeric_cols:\n",
    "            numeric_cols.remove('date')\n",
    "        \n",
    "        # Collapse duplicate dates\n",
    "        g_year = (g_year.groupby('date')\n",
    "                  .agg({**{c: 'mean' for c in numeric_cols},\n",
    "                        **{c: 'first' for c in g_year.columns.difference(numeric_cols + ['date'])}})\n",
    "                  .reset_index()\n",
    "                 )\n",
    "        \n",
    "        g_year = g_year.set_index('date').sort_index()\n",
    "        full_idx = pd.date_range(start=g_year.index.min(), end=g_year.index.max(), freq='D')\n",
    "        g_year = g_year.reindex(full_idx)\n",
    "        \n",
    "        g_year['plot_id'] = g_year['plot_id'].ffill().bfill()\n",
    "        for col in numeric_cols:\n",
    "            g_year[col] = g_year[col].interpolate(method='time', limit_direction='both')\n",
    "        \n",
    "        non_numeric = [c for c in g_year.columns if c not in numeric_cols + ['plot_id']]\n",
    "        if non_numeric:\n",
    "            g_year[non_numeric] = g_year[non_numeric].ffill().bfill()\n",
    "        \n",
    "        g_year = g_year.reset_index().rename(columns={'index':'date'})\n",
    "        daily_parts.append(g_year)\n",
    "    \n",
    "    return pd.concat(daily_parts, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b236a7-aa0e-4e24-9258-0d4cd62fc082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_daily = df.groupby('plot_id', as_index=False).apply(lambda g: stretch_to_daily_per_year(g)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f43e1-05cb-46b0-95f7-3df1e6de97a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_daily['week'] = full_daily['date'].dt.week\n",
    "\n",
    "full_daily['year'] = full_daily['date'].dt.year\n",
    "\n",
    "full_daily.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c2b1f-fac6-414e-b21d-0f9aded6b912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weekly_stats_list = []\n",
    "\n",
    "for (plot, year, week), group in tqdm(full_daily.groupby([full_daily['plot_id'], full_daily['year'], full_daily['week']])):\n",
    "    # print(plot, year,iiii week, group)\n",
    "    week_dict = {'plot_id': plot, 'year': year, 'week': week}\n",
    "    \n",
    "    # convert dates to numeric ordinals for slope calculation\n",
    "    x = group['date'].map(pd.Timestamp.toordinal).values\n",
    "    \n",
    "    for feat in veg_features:\n",
    "        y = group[feat].values\n",
    "        \n",
    "        # compute slope (direction matters)\n",
    "        week_dict[f'{feat}_slope'] = np.polyfit(x, y, 1)[0] if len(x) > 1 else np.nan\n",
    "        \n",
    "        # compute weekly summary statistics\n",
    "        week_dict[f'{feat}_mean'] = np.mean(y)\n",
    "        week_dict[f'{feat}_std']  = np.std(y)\n",
    "        week_dict[f'{feat}_max']  = np.max(y)\n",
    "        week_dict[f'{feat}_min']  = np.min(y)\n",
    "    \n",
    "    weekly_stats_list.append(week_dict)\n",
    "    # print(weekly_stats_list)\n",
    "    # break\n",
    "\n",
    "weekly_stats_df = pd.DataFrame(weekly_stats_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff420e-e32b-48ff-a996-d3228a75dee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wsdf = weekly_stats_df.copy()\n",
    "# wsdf = wsdf[wsdf['week'].isin([(i) for i in range(28, 45)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1712e1-495b-47ee-b280-dd6dfcd270f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "veg_agg = wsdf.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a0e96-ddde-422f-8143-b3e869a6d101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_cols = [col for col in veg_agg.columns if 'max' in col or 'min' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbe085-4994-4223-a30c-4ae52235acc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "veg_agg = veg_agg.drop(columns = drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf090b1-670a-456a-a90d-02e4eaa5a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_cols = [c for c in veg_agg.columns if c.endswith('_slope')]\n",
    "std_cols = [c for c in veg_agg.columns if c.endswith('_std')]\n",
    "\n",
    "veg_agg[slope_cols] = veg_agg[slope_cols].fillna(0)\n",
    "veg_agg[std_cols] = veg_agg[std_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f131a0-0ef1-4a0b-8d52-1ee808d26765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot so that each month becomes a set of columns\n",
    "veg_agg_wide = veg_agg.pivot_table(\n",
    "    index=['plot_id','year'],\n",
    "    columns='week',\n",
    "    values=[col for col in veg_agg.columns if col not in ['plot_id','year','week']]\n",
    ").reset_index()\n",
    "\n",
    "# Flatten multi-index columns\n",
    "veg_agg_wide.columns = ['_'.join([str(c) for c in col if c != '']) for col in veg_agg_wide.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ae072-305d-4fd5-9010-fdd172fe0bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi_cols = [col for col in veg_agg_wide.columns if col.startswith('ndvi_smooth_mean')]\n",
    "\n",
    "veg_agg_wide['ndvi_cov'] = veg_agg_wide[ndvi_cols].std(axis=1) / veg_agg_wide[ndvi_cols].mean(axis=1)\n",
    "veg_agg_wide['ndvi_mean'] = veg_agg_wide[ndvi_cols].mean(axis = 1)\n",
    "veg_agg_wide['ndvi_std'] = veg_agg_wide[ndvi_cols].std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd6eea-5fd7-4d35-a1bb-098087abb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_agg_wide.to_pickle('../data/ndvi/plots/final_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aeeec6-2717-4490-a047-5af858203c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 (GrapeExpectations)",
   "language": "python",
   "name": "grapeexpectations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
