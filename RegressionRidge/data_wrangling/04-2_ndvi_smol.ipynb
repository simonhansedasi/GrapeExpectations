{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81193f21-f4a2-462e-a98c-ff85b0464209",
   "metadata": {},
   "source": [
    "# ndvi_smol\n",
    "\n",
    "### Here we gather Sentinel-2 data to calculate plot ndvi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f59cb-a5b0-468a-841b-aba8a4ddcc4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # For Sentinel-2\n",
    "# # !pip install sentinelsat\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install sentinelsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58760e9-71ac-4d88-a952-905efeb58b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ee\n",
    "import geemap\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "# os.chdir('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "from scipy.stats import zscore\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208f481-cc07-4213-a754-55f8a90f594f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downloaded = False\n",
    "\n",
    "ndvi_files = [\n",
    "    'ndvi_2016-01-01_to_2016-12-31.csv',\n",
    "    'ndvi_2017-01-01_to_2017-12-31.csv',\n",
    "    'ndvi_2018-01-01_to_2018-12-31.csv',\n",
    "    'ndvi_2019-01-01_to_2019-12-31.csv',\n",
    "    'ndvi_2020-01-01_to_2020-12-31.csv',\n",
    "    'ndvi_2021-01-01_to_2021-12-31.csv',\n",
    "    'ndvi_2022-01-01_to_2022-12-31.csv',\n",
    "    'ndvi_2023-01-01_to_2023-12-31.csv',\n",
    "    'ndvi_2024-01-01_to_2024-12-31.csv',\n",
    "    \n",
    "]\n",
    "\n",
    "data_dir = '../data/ndvi/plots/'\n",
    "\n",
    "ndvi_file_paths = [os.path.join(data_dir, ndvi_file) for ndvi_file in ndvi_files]\n",
    "\n",
    "if all(os.path.isfile(path) for path in ndvi_file_paths):\n",
    "    downloaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d523c4a-8a54-4b63-b158-4a5828db4b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_indices(image):\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('ndvi')\n",
    "    evi = image.expression(\n",
    "        '2.5 * ((NIR - RED) / (NIR + 6*RED - 7.5*BLUE + 1))',\n",
    "        {'NIR': image.select('B8'), 'RED': image.select('B4'), 'BLUE': image.select('B2')}\n",
    "    ).rename('evi')\n",
    "    ndwi = image.normalizedDifference(['B8', 'B11']).rename('ndwi')\n",
    "    savi = image.expression(\n",
    "        '((NIR - RED) / (NIR + RED + 0.5)) * 1.5',\n",
    "        {'NIR': image.select('B8'), 'RED': image.select('B4')}\n",
    "    ).rename('savi')\n",
    "    rendvi = image.normalizedDifference(['B8', 'B5']).rename('rendvi')\n",
    "    mcari2 = image.expression(\n",
    "        '((NIR - RE) - 0.2*(NIR - RED)) * (NIR / RE)',\n",
    "        {'NIR': image.select('B8'), 'RED': image.select('B4'), 'RE': image.select('B5')}\n",
    "    ).rename('mcari2')\n",
    "    \n",
    "    return image.addBands([ndvi, evi, ndwi, savi, rendvi, mcari2])\n",
    "\n",
    "\n",
    "\n",
    "def mask_clouds(img, cloud_prob_threshold=20):\n",
    "    '''Mask clouds using s2cloudless probability band.'''\n",
    "    cloud_prob = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY') \\\n",
    "        .filter(ee.Filter.eq('system:index', img.get('system:index'))).first()\n",
    "    clouds = cloud_prob.gt(cloud_prob_threshold)\n",
    "    mask = clouds.Not()\n",
    "    return img.updateMask(mask)\n",
    "\n",
    "\n",
    "def reduce_region(img, geom, plot_id):\n",
    "    # all EE operations\n",
    "    stats = img.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=geom,\n",
    "        scale=10,\n",
    "        bestEffort=True,\n",
    "        maxPixels=1e13\n",
    "    )\n",
    "    props = {k: stats.get(k) for k in [\n",
    "        'ndvi',#'evi','ndwi','savi','rendvi','mcari2'\n",
    "    ]}\n",
    "    props['date'] = img.date().format('YYYY-MM-dd')\n",
    "    props['plot_id'] = plot_id\n",
    "    return ee.Feature(None, props)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1873afa-bff4-421e-a15d-f4692349815a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Here we initialize our access to the satellite data download. \n",
    "## Requires authentication via browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee2f25-15c6-4bb9-b659-a1b7b8780573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if downloaded == False:\n",
    "    \n",
    "    \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "    # load the vineyard polygon (GeoJSON)\n",
    "    vineyard = pd.read_pickle('../data/polygons/RegressionRidge_smol_smol.pkl')\n",
    "\n",
    "\n",
    "    from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "    # your existing GeoDataFrame\n",
    "    gdf = vineyard.copy()\n",
    "\n",
    "    rows = []\n",
    "    for idx, row in gdf.iterrows():\n",
    "        geom = row.geometry\n",
    "        parent = row['plot_id']\n",
    "\n",
    "        if isinstance(geom, Polygon):\n",
    "            rows.append({\"geometry\": geom, \"parent_plot\": parent})\n",
    "        elif isinstance(geom, MultiPolygon):\n",
    "            for poly in geom.geoms:\n",
    "                rows.append({\"geometry\": poly, \"parent_plot\": parent})\n",
    "\n",
    "    # create new flattened GeoDataFrame\n",
    "    vineyard_flat = gpd.GeoDataFrame(rows, crs=gdf.crs)\n",
    "\n",
    "\n",
    "\n",
    "    geoms = [ee.Geometry.Polygon(list(p.exterior.coords)) for p in vineyard_flat.geometry]\n",
    "    \n",
    "    \n",
    "if downloaded == True:\n",
    "    print('data already downloaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37779e75-ebaf-49dd-a2a2-f6c72f2d8b87",
   "metadata": {},
   "source": [
    "### The below code will loop over a given set of years starting in 2016 (earliest available data), search for the polygon coordinates and compute ndvi for our plots. Finally, the data is serialized as a pandas pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c434b89-275e-40d1-8dd5-91955d1b55ce",
   "metadata": {},
   "source": [
    "Set dates to search over. Make sure there is a 'plots' folder for the plot ndvi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d96b1-2a31-4dfd-9c10-2af9a5f7ddc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if downloaded == False:\n",
    "    years = [str(year) for year in range(2016, 2025)]\n",
    "    months = ['01', '12']\n",
    "    days = ['01', '31']\n",
    "    start_dates = [f'{year}-{months[0]}-{days[0]}' for year in years]\n",
    "    end_dates   = [f'{year}-{months[1]}-{days[1]}' for year in years]\n",
    "\n",
    "    os.makedirs('data/ndvi/plots', exist_ok=True)\n",
    "    \n",
    "    \n",
    "if downloaded == True:\n",
    "    print('already been here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57ce8d-543e-449d-8423-0042534f9d19",
   "metadata": {},
   "source": [
    "Loop over years and search database for images overlapping our polygons. Then download the and st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb00070-1dff-459a-b667-affbaf545dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if downloaded == False:\n",
    "\n",
    "    # --- Flatten MultiPolygons if necessary ---\n",
    "    def shapely_to_ee_feature(poly, plot_id):\n",
    "        \"\"\"Convert a Polygon or MultiPolygon to EE Feature with plot_id property.\"\"\"\n",
    "        if poly.geom_type == 'Polygon':\n",
    "            return ee.Feature(ee.Geometry.Polygon(list(poly.exterior.coords)), {'plot_id': plot_id})\n",
    "        elif poly.geom_type == 'MultiPolygon':\n",
    "            features = []\n",
    "            for p in poly.geoms:\n",
    "                features.append(ee.Feature(ee.Geometry.Polygon(list(p.exterior.coords)), {'plot_id': plot_id}))\n",
    "            return ee.FeatureCollection(features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported geometry type: {poly.geom_type}\")\n",
    "            \n",
    "            \n",
    "if downloaded == True:\n",
    "    print('polygons already flattened')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff69d25-95ff-4250-8319-142d0ac95ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if downloaded == False:\n",
    "    # Flatten GeoDataFrame into FeatureCollection\n",
    "    ee_features = []\n",
    "    for idx, row in tqdm(vineyard_flat.iterrows()):\n",
    "        poly = row.geometry\n",
    "        parent = row.get('parent_plot', f'plot_{idx}')\n",
    "        feat = shapely_to_ee_feature(poly, parent)\n",
    "\n",
    "        if isinstance(feat, ee.FeatureCollection):\n",
    "            ee_features.extend(feat.toList(feat.size()).getInfo())\n",
    "        else:\n",
    "            ee_features.append(feat.getInfo())\n",
    "\n",
    "    # hex_fc['geometry'] = hex_fc.geometry.buffer(5)  # buffer by 5 meters\n",
    "\n",
    "    hex_fc = ee.FeatureCollection(ee_features)\n",
    "    \n",
    "if downloaded == True:\n",
    "    print('already done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce751d3-9b30-4454-92fc-bfbaee6d95ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if downloaded == False:\n",
    "    for start_date, end_date in zip(start_dates, end_dates):\n",
    "        break\n",
    "        '''\n",
    "        Undo the break statement to download the data\n",
    "        '''\n",
    "\n",
    "        # fname = f'../data/ndvi/plots/ndvi_{start_date}_to_{end_date}.pkl'\n",
    "        # if os.path.isfile(fname):\n",
    "        #     continue\n",
    "\n",
    "        # Load Sentinel-2 collection and add indices\n",
    "        collection = (\n",
    "            ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "            .filterBounds(hex_fc)\n",
    "            .filterDate(start_date, end_date)\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
    "            .select(['B2', 'B4', 'B5', 'B8', 'B11'])\n",
    "            .map(add_indices)\n",
    "        )\n",
    "\n",
    "        # Server-side extraction: map reduce_region over each hex for each image\n",
    "        def extract_features(img):\n",
    "            return hex_fc.map(\n",
    "                lambda f: reduce_region(\n",
    "                    img, \n",
    "                    f.geometry(), \n",
    "                    f.get('plot_id')\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Flatten all features across all images\n",
    "        fc = collection.map(extract_features).flatten()\n",
    "\n",
    "        # Export to Google Drive as CSV to avoid memory issues\n",
    "        task = ee.batch.Export.table.toDrive(\n",
    "            collection=fc,\n",
    "            description=f'ndvi_{start_date}_to_{end_date}',\n",
    "            folder=f'ee_ndvi_exports_{start_date}_to_{end_date}',\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        task.start()\n",
    "        print(f\"Export started for {start_date} to {end_date}. Check Google Drive folder 'ee_ndvi_exports'.\")\n",
    "if downloaded == True:\n",
    "    print('data already exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27c3af-3a8e-45cf-af5c-40c0d1bd042a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "code to check on earth engine task status\n",
    "'''\n",
    "# import ee\n",
    "# ee.Initialize()\n",
    "\n",
    "# tasks = ee.batch.Task.list()\n",
    "\n",
    "# for t in tasks:\n",
    "#     print(f\"ID: {t.id}\")\n",
    "#     print(f\"Type: {t.task_type}\")\n",
    "#     print(f\"Description: {t.config.get('description')}\")\n",
    "#     print(f\"State: {t.state}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ac901-293e-4d23-bd3c-b87cb9e47508",
   "metadata": {},
   "source": [
    "# We have manually downloaded .CSV files from google drive. Now we will manually put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4bdab-2911-437f-8405-91812f645092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dfs = []\n",
    "data_path = '../data/ndvi/plots/'\n",
    "\n",
    "for file_path in tqdm(ndvi_files):\n",
    "\n",
    "    ndvi_path = os.path.join(data_path, file_path)\n",
    "    df = pd.read_csv(ndvi_path)\n",
    "    df = df[['date','ndvi', 'plot_id']]\n",
    "    \n",
    "    dfs.append(df)\n",
    "    \n",
    "df = pd.concat(dfs, axis = 0)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6592b-49ed-4868-a2d9-13a293491544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e606ff6-fc34-4db7-80f2-aa21116a05b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hampel_filter_pth = '../data/ndvi/plots/hampel_filtered.pkl'\n",
    "\n",
    "if not os.path.isfile(hampel_filter_pth):\n",
    "    def hampel_filter_series(s, window=7, n_sigmas=3):\n",
    "        \"\"\"\n",
    "        Hampel filter for 1D pandas Series.\n",
    "\n",
    "        Parameters:\n",
    "        - s: pd.Series, time-ordered\n",
    "        - window: half-window size (total window = 2*window+1)\n",
    "        - n_sigmas: threshold multiplier\n",
    "\n",
    "        Returns:\n",
    "        - pd.Series with outliers replaced by NaN\n",
    "        \"\"\"\n",
    "        k = 1.4826  # scale factor for MAD\n",
    "        s_clean = s.copy()\n",
    "\n",
    "        # rolling median and MAD\n",
    "        rolling_med = s.rolling(window=2*window+1, center=True, min_periods=1).median()\n",
    "        rolling_mad = s.rolling(window=2*window+1, center=True, min_periods=1).apply(\n",
    "            lambda x: np.median(np.abs(x - np.median(x))), raw=True\n",
    "        )\n",
    "\n",
    "        # avoid zero MAD\n",
    "        rolling_mad[rolling_mad == 0] = 1e-6\n",
    "\n",
    "        # compute deviations\n",
    "        diff = np.abs(s - rolling_med)\n",
    "        threshold = n_sigmas * k * rolling_mad\n",
    "\n",
    "        # replace outliers with NaN\n",
    "        s_clean[diff > threshold] = np.nan\n",
    "        return s_clean\n",
    "\n",
    "    # list of columns to filter\n",
    "    cols_to_filter = [\n",
    "        'ndvi', #'evi', 'ndwi', 'savi', 'rendvi', 'mcari2'\n",
    "    ]\n",
    "\n",
    "    for col in tqdm(cols_to_filter):\n",
    "        # apply Hampel filter group-wise\n",
    "        df[col] = df.groupby('plot_id')[col].transform(\n",
    "            lambda s: hampel_filter_series(s, window=7, n_sigmas=2)\n",
    "        )\n",
    "\n",
    "        # smooth filtered series\n",
    "        df[f'{col}_smooth'] = df.groupby('plot_id')[col].transform(\n",
    "            lambda s: s.rolling(window=7, center=True, min_periods=1).mean()\n",
    "        )\n",
    "    df.to_pickle(hampel_filter_pth)\n",
    "    print(\"Hampel filter and smoothing done!\")\n",
    "if os.path.isfile(hampel_filter_pth):\n",
    "    print('file is here')\n",
    "    df = pd.read_pickle(hampel_filter_pth)\n",
    "\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ecc52-7480-49fa-a0ec-c5ade8230d5f",
   "metadata": {},
   "source": [
    "### Let's do a quick stats check here. Figure out how confident we are in these smoothed curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52afc3-a76f-46c0-948c-0c51f1b683b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602ec78-560b-4bd8-9c85-8317963beacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify your index columns\n",
    "id_cols = [\"plot_id\", \"date\"]\n",
    "\n",
    "# Raw indices\n",
    "raw_cols = [\n",
    "    \"ndvi\",#\"evi\",\"ndwi\",\"savi\",\"rendvi\",\"mcari2\"\n",
    "]\n",
    "\n",
    "# Smooth versions\n",
    "smooth_cols = ['ndvi_smooth']#[c+\"_smooth\" for c in raw_cols]\n",
    "\n",
    "# Step 1: Fill raw with smoothed, then median per plot, then global median\n",
    "for raw, smooth in zip(raw_cols, smooth_cols):\n",
    "    \n",
    "    df[raw] = df[raw].fillna(df[smooth])  # use smoothed\n",
    "    df[raw] = df.groupby(\"plot_id\")[raw].transform(lambda x: x.fillna(x.median()))\n",
    "    df[raw] = df[raw].fillna(df[raw].median())\n",
    "\n",
    "# Step 2: Fill smooth (only a few NaNs)\n",
    "for col in smooth_cols:\n",
    "    df[col] = df.groupby(\"plot_id\")[col].transform(lambda x: x.fillna(x.median()))\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Step 3: Recompute residuals\n",
    "for raw, smooth in zip(raw_cols, smooth_cols):\n",
    "    resid = raw + \"_residual\"\n",
    "    resid_pct = raw + \"_residual_pct\"\n",
    "    df[resid] = df[raw] - df[smooth]\n",
    "    # To avoid divide-by-zero, use np.where\n",
    "    df[resid_pct] = np.where(df[smooth] != 0, df[resid] / df[smooth], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e8f31-229e-46c7-b9b9-303fadb6e280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89d606-b3a9-421a-ad0b-e5a21eba11d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_id = 23    # example plot\n",
    "year = 2024    # example year\n",
    "\n",
    "df_plot = df[(df['plot_id']==plot_id) & (df['date'].dt.year==year)].sort_values('date')\n",
    "\n",
    "indices = [\n",
    "    'ndvi',#'evi','ndwi','savi','rendvi','mcari2'\n",
    "]\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "for idx in indices:\n",
    "    plt.plot(df_plot['date'], df_plot[idx], 'o-', alpha=0.5, label=f'{idx} raw')\n",
    "    sm = idx + '_smooth'\n",
    "    if sm in df_plot.columns:\n",
    "        plt.plot(df_plot['date'], df_plot[sm], '-', lw=2, label=f'{idx} smooth')\n",
    "    plt.title(f'Plot {plot_id} - {year} index time series')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Index value')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099eb9fa-4c9e-479c-b2b9-6e45ab7b5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_features = [\n",
    "    'ndvi_smooth',\n",
    "                # 'evi_smooth', 'ndwi_smooth', 'savi_smooth', 'rendvi_smooth', 'mcari2_smooth'\n",
    "]\n",
    "\n",
    "df['month'] = df['date'].dt.month\n",
    "df['week'] = df['date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7541dc-b109-44d3-a59d-abddd02e205c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcae87-be49-44c9-bbc3-ce687412e8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a4490-65a0-43ab-954d-6bffdd8bf13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f3db9-693a-4796-ba37-4c9e41611a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stretched_pth = '../data/ndvi/plots/ndvi_stretch_daily.pkl'\n",
    "if not os.path.isfile(stretched_pth):\n",
    "    print('stretching df')\n",
    "    def stretch_to_daily_per_year(g):\n",
    "        # g is all data for one plot_id\n",
    "        daily_parts = []\n",
    "        for year, g_year in (g.groupby(g['date'].dt.year)):\n",
    "            g_year = g_year.copy()\n",
    "            numeric_cols = g_year.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if 'date' in numeric_cols:\n",
    "                numeric_cols.remove('date')\n",
    "\n",
    "            # Collapse duplicate dates\n",
    "            g_year = (g_year.groupby('date')\n",
    "                      .agg({**{c: 'mean' for c in numeric_cols},\n",
    "                            **{c: 'first' for c in g_year.columns.difference(numeric_cols + ['date'])}})\n",
    "                      .reset_index()\n",
    "                     )\n",
    "\n",
    "            g_year = g_year.set_index('date').sort_index()\n",
    "            full_idx = pd.date_range(start=g_year.index.min(), end=g_year.index.max(), freq='D')\n",
    "            g_year = g_year.reindex(full_idx)\n",
    "\n",
    "            g_year['plot_id'] = g_year['plot_id'].ffill().bfill()\n",
    "            for col in numeric_cols:\n",
    "                g_year[col] = g_year[col].interpolate(method='time', limit_direction='both')\n",
    "\n",
    "            non_numeric = [c for c in g_year.columns if c not in numeric_cols + ['plot_id']]\n",
    "            if non_numeric:\n",
    "                g_year[non_numeric] = g_year[non_numeric].ffill().bfill()\n",
    "\n",
    "            g_year = g_year.reset_index().rename(columns={'index':'date'})\n",
    "            daily_parts.append(g_year)\n",
    "\n",
    "        return pd.concat(daily_parts, axis=0)\n",
    "\n",
    "\n",
    "    full_daily = df.groupby(\n",
    "        'plot_id', as_index=False\n",
    "    ).apply(lambda g: stretch_to_daily_per_year(g)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    full_daily['week'] = full_daily['date'].dt.week\n",
    "\n",
    "    full_daily['year'] = full_daily['date'].dt.year\n",
    "\n",
    "    full_daily.sort_values('date')\n",
    "\n",
    "    full_daily.to_pickle(stretched_pth)\n",
    "if os.path.isfile(stretched_pth):\n",
    "    print('df already stretched')\n",
    "    df = pd.read_pickle(stretched_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01880f77-3ef5-438c-a942-f7f6c9a84ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interp_pth = '../data/ndvi/plots/ndvi_daily_interp.pkl'\n",
    "\n",
    "\n",
    "if not os.path.isfile(interp_pth):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    full_range = pd.date_range('2016-01-01', '2024-12-31', freq='D')\n",
    "\n",
    "\n",
    "    df = df.sort_values(['plot_id', 'date'])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for pid, g in tqdm(df.groupby('plot_id', sort=False)):\n",
    "        g = g.set_index('date').reindex(full_range)   # only one plot at a time\n",
    "\n",
    "        # Set index to date (required for resample + time interpolation)\n",
    "\n",
    "        # Resample daily â†’ insert missing days\n",
    "        g = g.resample('D').asfreq()\n",
    "\n",
    "        # Reattach plot_id\n",
    "        g['plot_id'] = pid\n",
    "\n",
    "        # Interpolate NDVI\n",
    "        g['ndvi_smooth_interp'] = g['ndvi_smooth'].interpolate(\n",
    "            method='time',\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "        results.append(g)\n",
    "\n",
    "    # Combine all plots\n",
    "    daily = pd.concat(results).reset_index()\n",
    "    daily['month'] = daily['index'].dt.month\n",
    "    daily['year'] = daily['index'].dt.year\n",
    "    daily['week'] = daily['index'].dt.week\n",
    "\n",
    "    daily.to_pickle(interp_pth)\n",
    "\n",
    "if os.path.isfile(interp_pth):\n",
    "    daily = pd.read_pickle(interp_pth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f4d26-f594-49fd-822d-22c226cab06a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd11f6-5c4a-45ec-bc2a-30824fcde452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ed371-5e0e-486f-b426-2f6c2bff4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_daily = daily.copy()\n",
    "\n",
    "# full_daily = full_daily[\n",
    "#     (full_daily['week'] > 24) &\n",
    "#     (full_daily['week'] < 46)\n",
    "# ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e22ab-b8fc-4e9d-9a8a-126d7635ca1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32705acb-dd37-4a7e-ae4a-896b9e1ad956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076983e8-1167-42a5-8143-3c0ebe69ad83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_daily_0 = full_daily[full_daily['plot_id'] == 0].copy()\n",
    "\n",
    "plt.plot(full_daily_0['ndvi_smooth_interp'], alpha = 0.6,color = 'red', label = 'Interpolation')\n",
    "\n",
    "plt.plot(full_daily_0['ndvi_smooth'], alpha = 0.6, color = 'blue', label = 'Smoothed Data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87634a-130c-4019-af2d-6158628a730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c2b1f-fac6-414e-b21d-0f9aded6b912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weekly_stats_list = []\n",
    "\n",
    "for (plot, year, week), group in tqdm(\n",
    "    full_daily.groupby([full_daily['plot_id'], full_daily['year'], full_daily['week']])\n",
    "):\n",
    "    # print(plot, year,iiii week, group)\n",
    "    week_dict = {'plot_id': plot, 'year': year, 'week': week}\n",
    "    \n",
    "    # convert dates to numeric ordinals for slope calculation\n",
    "    x = group['date'].map(pd.Timestamp.toordinal).values\n",
    "    \n",
    "    for feat in ['ndvi_smooth']:\n",
    "        y = group[feat].values\n",
    "        \n",
    "        # compute slope (direction matters)\n",
    "        week_dict[f'{feat}_slope'] = np.polyfit(x, y, 1)[0] if len(x) > 1 else np.nan\n",
    "        \n",
    "        # compute weekly summary statistics\n",
    "        week_dict[f'{feat}_mean'] = np.mean(y)\n",
    "        week_dict[f'{feat}_std']  = np.std(y)\n",
    "        week_dict[f'{feat}_max']  = np.max(y)\n",
    "        week_dict[f'{feat}_min']  = np.min(y)\n",
    "    \n",
    "    weekly_stats_list.append(week_dict)\n",
    "    # print(weekly_stats_list)\n",
    "    # break\n",
    "\n",
    "weekly_stats_df = pd.DataFrame(weekly_stats_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff420e-e32b-48ff-a996-d3228a75dee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wsdf = weekly_stats_df.copy()\n",
    "# wsdf = wsdf[wsdf['week'].isin([(i) for i in range(28, 45)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1712e1-495b-47ee-b280-dd6dfcd270f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "veg_agg = wsdf.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a0e96-ddde-422f-8143-b3e869a6d101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_cols = [col for col in veg_agg.columns if 'max' in col or 'min' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbe085-4994-4223-a30c-4ae52235acc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "veg_agg = veg_agg.drop(columns = drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf090b1-670a-456a-a90d-02e4eaa5a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_cols = [c for c in veg_agg.columns if c.endswith('_slope')]\n",
    "std_cols = [c for c in veg_agg.columns if c.endswith('_std')]\n",
    "\n",
    "veg_agg[slope_cols] = veg_agg[slope_cols].fillna(0)\n",
    "veg_agg[std_cols] = veg_agg[std_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f131a0-0ef1-4a0b-8d52-1ee808d26765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot so that each month becomes a set of columns\n",
    "veg_agg_wide = veg_agg.pivot_table(\n",
    "    index=['plot_id','year'],\n",
    "    columns='week',\n",
    "    values=[col for col in veg_agg.columns if col not in ['plot_id','year','week']]\n",
    ").reset_index()\n",
    "\n",
    "# Flatten multi-index columns\n",
    "veg_agg_wide.columns = ['_'.join([str(c) for c in col if c != '']) for col in veg_agg_wide.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ae072-305d-4fd5-9010-fdd172fe0bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi_cols = [col for col in veg_agg_wide.columns if col.startswith('ndvi_smooth_mean')]\n",
    "\n",
    "veg_agg_wide['ndvi_cov'] = veg_agg_wide[ndvi_cols].std(axis=1) / veg_agg_wide[ndvi_cols].mean(axis=1)\n",
    "veg_agg_wide['ndvi_mean'] = veg_agg_wide[ndvi_cols].mean(axis = 1)\n",
    "veg_agg_wide['ndvi_std'] = veg_agg_wide[ndvi_cols].std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d95004-ff56-479c-b30d-f18e3ba78fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mu_min, mu_max = veg_agg_wide['ndvi_mean'].min(), veg_agg_wide['ndvi_mean'].max()\n",
    "sigma_min, sigma_max = veg_agg_wide['ndvi_std'].min(), veg_agg_wide['ndvi_std'].max()\n",
    "cov_min, cov_max = veg_agg_wide['ndvi_cov'].min(), veg_agg_wide['ndvi_cov'].max()\n",
    "\n",
    "veg_agg_wide['ndvi_mean_norm'] = (veg_agg_wide['ndvi_mean'] - mu_min) / (mu_max - mu_min)\n",
    "\n",
    "# reverse because higher sigma is worse\n",
    "veg_agg_wide['ndvi_std_norm'] = (sigma_max - veg_agg_wide['ndvi_std']) / (sigma_max - sigma_min)\n",
    "\n",
    "# reverse because higher cov is worse\n",
    "veg_agg_wide['ndvi_cov_norm'] = (cov_max - veg_agg_wide['ndvi_cov']) / (cov_max - cov_min)\n",
    "\n",
    "# 2. Weighted geometric mean health score\n",
    "w1, w2, w3 = 0.5, 0.25, 0.25\n",
    "\n",
    "df['health'] = (\n",
    "    (df['ndvi_mean_norm'] ** w1) *\n",
    "    (df['ndvi_sigma_norm'] ** w2) *\n",
    "    (df['ndvi_cov_norm'] ** w3)\n",
    ") ** (1 / (w1 + w2 + w3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c23be1-1a0b-41f9-a6a5-584c2beff663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd6eea-5fd7-4d35-a1bb-098087abb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_agg_wide.to_pickle('../data/ndvi/plots/final_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aeeec6-2717-4490-a047-5af858203c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 (GrapeExpectations)",
   "language": "python",
   "name": "grapeexpectations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
